{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "f99d570b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… GPU Found: NVIDIA GeForce RTX 2050\n",
                        "Memory: 3.68 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    device_name = torch.cuda.get_device_name(0)\n",
                "    # The correct way to get memory in PyTorch\n",
                "    total_mem = torch.cuda.get_device_properties(0).total_memory\n",
                "    \n",
                "    print(f\"âœ… GPU Found: {device_name}\")\n",
                "    print(f\"Memory: {total_mem / 1024**3:.2f} GB\")\n",
                "else:\n",
                "    print(\"âŒ GPU NOT found. Check your drivers or Secure Boot.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "4fff6786",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Vocabulary Analysis (Optional)\n",
                "# # Count token frequencies in the English subset\n",
                "# token_counts = Counter()\n",
                "\n",
                "# print(\"Analyzing vocabulary usage...\")\n",
                "# # Sample a subset if dataset is huge, but 300k is manageable or take first 10k\n",
                "# for i, example in enumerate(dataset['train']):\n",
                "#     if i > 10000: break # Analyze first 10k samples for speed\n",
                "#     ids = tokenizer(example['source_text'], truncation=False, add_special_tokens=False)['input_ids']\n",
                "#     token_counts.update(ids)\n",
                "\n",
                "# print(f\"Total unique tokens used (in sample): {len(token_counts)}\")\n",
                "# print(f\"Vocab size of tokenizer: {tokenizer.vocab_size}\")\n",
                "\n",
                "# # Top 20 tokens\n",
                "# top_20 = token_counts.most_common(20)\n",
                "# print(\"Top 20 tokens:\")\n",
                "# for tid, count in top_20:\n",
                "#     print(f\"{tokenizer.decode([tid])}: {count}\")\n",
                "\n",
                "# # Tokens not used\n",
                "# unused_count = tokenizer.vocab_size - len(token_counts)\n",
                "# print(f\"Unused tokens (approx): {unused_count} ({unused_count/tokenizer.vocab_size:.2%})\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "91edcba9",
            "metadata": {},
            "source": [
                "# PII Masking Model Pipeline (Browser Optimized)\n",
                "\n",
                "This notebook implements a pipeline to create a small, efficient PII masking model using the Lottery Ticket Hypothesis (LTH) and Quantization.\n",
                "\n",
                "**Key Steps:**\n",
                "1. **Load Data**: `ai4privacy/pii-masking-300k` (Filter for **English** only).\n",
                "2. **Preprocessing**: Robust tokenization using character offsets to handle dataset quirks.\n",
                "3. **Save Initial Weights**: Critical for LTH \"rewinding\".\n",
                "4. **Fine-tune** -> **Prune** -> **Reset** -> **Retrain** -> **Quantize**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "id": "d213160f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install transformers datasets torch optimum onnxruntime onnx scikit-learn numpy pandas accelerate evaluate seqeval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "1ddd54dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "import evaluate\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForTokenClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorForTokenClassification\n",
                ")\n",
                "from torch.nn.utils import prune\n",
                "from collections import Counter\n",
                "\n",
                "# Configuration\n",
                "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
                "DATASET_NAME = \"ai4privacy/pii-masking-300k\"\n",
                "OUTPUT_DIR = \"./pii_model_output\"\n",
                "INITIAL_WEIGHTS_PATH = os.path.join(OUTPUT_DIR, \"initial_weights.pt\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52b6e6cc",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Splits\n",
                "We load the dataset and filter for **English** (`language == 'English'`).\n",
                "Since the dataset only provides `train` and `validation` splits, we split `validation` into disjoint `validation` and `test` sets (50/50)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "id": "49415fab",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`trust_remote_code` is not supported anymore.\n",
                        "Please check that the Hugging Face dataset 'ai4privacy/pii-masking-300k' isn't based on a loading script and remove `trust_remote_code`.\n",
                        "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating Test split from Validation...\n",
                        "DatasetDict({\n",
                        "    train: Dataset({\n",
                        "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
                        "        num_rows: 29908\n",
                        "    })\n",
                        "    validation: Dataset({\n",
                        "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
                        "        num_rows: 3973\n",
                        "    })\n",
                        "    test: Dataset({\n",
                        "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
                        "        num_rows: 3973\n",
                        "    })\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "# Load Dataset\n",
                "# Note: 'trust_remote_code=True' needed for some HF datasets, beneficial to keep.\n",
                "dataset = load_dataset(DATASET_NAME, trust_remote_code=True)\n",
                "\n",
                "# Filter for English. Verified key is 'language' == 'English'.\n",
                "dataset = dataset.filter(lambda x: x[\"language\"] == \"English\")\n",
                "\n",
                "# Check splits. If 'test' exists, use it. Else split validation.\n",
                "if \"test\" not in dataset:\n",
                "    print(\"Creating Test split from Validation...\")\n",
                "    val_test_split = dataset[\"validation\"].train_test_split(test_size=0.5, seed=42)\n",
                "    dataset[\"validation\"] = val_test_split[\"train\"]\n",
                "    dataset[\"test\"] = val_test_split[\"test\"]\n",
                "\n",
                "print(dataset)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a345014d",
            "metadata": {},
            "source": [
                "## 2. Parsing & Label Extraction\n",
                "The dataset stores complex fields (`privacy_mask`) as JSON strings (in some versions) or list objects. We ensure they are parsed and extract unique labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "76ea781e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting unique labels from Train split...\n",
                        "Unique labels: 57\n"
                    ]
                }
            ],
            "source": [
                "def parse_dataset_row(example):\n",
                "    if isinstance(example['privacy_mask'], str):\n",
                "        example['privacy_mask'] = json.loads(example['privacy_mask'])\n",
                "    return example\n",
                "\n",
                "# We map once. This is cached by HF datasets so subsequent calls are fast.\n",
                "dataset = dataset.map(parse_dataset_row, num_proc=4)\n",
                "\n",
                "print(\"Extracting unique labels from Train split...\")\n",
                "unique_labels = set()\n",
                "# Use a generator for memory efficiency\n",
                "for privacy_mask in dataset['train']['privacy_mask']:\n",
                "    for entity in privacy_mask:\n",
                "        unique_labels.add(entity['label'])\n",
                "\n",
                "label_list = [\"O\"]\n",
                "for label in sorted(list(unique_labels)):\n",
                "    label_list.append(f\"B-{label}\")\n",
                "    label_list.append(f\"I-{label}\")\n",
                "\n",
                "label2id = {label: i for i, label in enumerate(label_list)}\n",
                "id2label = {i: label for label, i in label2id.items()}\n",
                "print(f\"Unique labels: {len(label_list)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "597d5a15",
            "metadata": {},
            "source": [
                "## 3. Robust Tokenization & Alignment\n",
                "We use `return_offsets_mapping=True` to map directly from character spans in `privacy_mask` to tokenizer tokens."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "id": "f9fd60c4",
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
                "\n",
                "def tokenize_and_align_labels(examples):\n",
                "    tokenized_inputs = tokenizer(\n",
                "        examples[\"source_text\"],\n",
                "        truncation=True,\n",
                "        max_length=512,\n",
                "        stride=64,\n",
                "        return_overflowing_tokens=True,\n",
                "        return_offsets_mapping=True,\n",
                "        padding=False # Dynamic padding in DataCollator\n",
                "    )\n",
                "\n",
                "    labels = []\n",
                "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
                "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
                "\n",
                "    for i, offsets in enumerate(offset_mapping):\n",
                "        sample_idx = sample_mapping[i]\n",
                "        mask_list = examples[\"privacy_mask\"][sample_idx]\n",
                "        \n",
                "        # Initialize with O\n",
                "        chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][i])\n",
                "        \n",
                "        for entity in mask_list:\n",
                "            label_type = entity[\"label\"]\n",
                "            start_char = entity[\"start\"]\n",
                "            end_char = entity[\"end\"]\n",
                "            \n",
                "            b_id = label2id.get(f\"B-{label_type}\")\n",
                "            i_id = label2id.get(f\"I-{label_type}\")\n",
                "            if b_id is None: continue\n",
                "            \n",
                "            # Find all overlapping tokens for this entity in this chunk\n",
                "            # Overlap condition: not (token_end <= entity_start or token_start >= entity_end)\n",
                "            overlapping_indices = []\n",
                "            for idx, (t_start, t_end) in enumerate(offsets):\n",
                "                if t_start == 0 and t_end == 0: continue\n",
                "                if not (t_end <= start_char or t_start >= end_char):\n",
                "                    overlapping_indices.append(idx)\n",
                "            \n",
                "            # Assign Labels\n",
                "            for k, idx in enumerate(overlapping_indices):\n",
                "                current_label = chunk_labels[idx]\n",
                "                # Don't overwrite existing B/I labels unless they are O or -100\n",
                "                if current_label != label2id[\"O\"] and current_label != -100:\n",
                "                     continue\n",
                "                \n",
                "                # Logic: The FIRST overlapping token gets B, others I.\n",
                "                # UNLESS the entity actually started before this token (Sliding window split).\n",
                "                if k == 0:\n",
                "                    t_start = offsets[idx][0]\n",
                "                    # If entity start is >= token start, it means this token contains the start -> B\n",
                "                    if start_char >= t_start:\n",
                "                        chunk_labels[idx] = b_id\n",
                "                    else:\n",
                "                        # Entity started before this token -> I\n",
                "                        chunk_labels[idx] = i_id\n",
                "                else:\n",
                "                    chunk_labels[idx] = i_id\n",
                "        \n",
                "        # Mask special tokens\n",
                "        for idx, (t_start, t_end) in enumerate(offsets):\n",
                "            if t_start == 0 and t_end == 0:\n",
                "                chunk_labels[idx] = -100\n",
                "        \n",
                "        labels.append(chunk_labels)\n",
                "\n",
                "    tokenized_inputs[\"labels\"] = labels\n",
                "    return tokenized_inputs\n",
                "\n",
                "tokenized_datasets = dataset.map(\n",
                "    tokenize_and_align_labels, \n",
                "    batched=True, \n",
                "    remove_columns=dataset[\"train\"].column_names,\n",
                "    num_proc=4\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40c333fd",
            "metadata": {},
            "source": [
                "## 4. Save Initial Weights\n",
                "Save the untrained (or pre-trained base) weights to support the Lottery Ticket Hypothesis rewinding step later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "id": "437618ba",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 266.58it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
                        "\u001b[1mDistilBertForTokenClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
                        "Key                     | Status     | \n",
                        "------------------------+------------+-\n",
                        "vocab_transform.bias    | UNEXPECTED | \n",
                        "vocab_projector.bias    | UNEXPECTED | \n",
                        "vocab_transform.weight  | UNEXPECTED | \n",
                        "vocab_layer_norm.weight | UNEXPECTED | \n",
                        "vocab_layer_norm.bias   | UNEXPECTED | \n",
                        "classifier.bias         | MISSING    | \n",
                        "classifier.weight       | MISSING    | \n",
                        "\n",
                        "\u001b[3mNotes:\n",
                        "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
                        "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initial weights saved to ./pii_model_output/initial_weights.pt with 57 labels and sliding window configuration.\n"
                    ]
                }
            ],
            "source": [
                "model = AutoModelForTokenClassification.from_pretrained(\n",
                "    MODEL_CHECKPOINT, \n",
                "    # Pass the new label map\n",
                "    num_labels=len(label_list), \n",
                "    id2label=id2label, \n",
                "    label2id=label2id\n",
                ")\n",
                "\n",
                "torch.save(model.state_dict(), INITIAL_WEIGHTS_PATH)\n",
                "print(f\"Initial weights saved to {INITIAL_WEIGHTS_PATH} with {len(label_list)} labels and sliding window configuration.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "id": "8323ee12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation Metrics\n",
                "metric = evaluate.load(\"seqeval\")\n",
                "\n",
                "def compute_metrics(p):\n",
                "    predictions, labels = p\n",
                "    predictions = np.argmax(predictions, axis=2)\n",
                "\n",
                "    # Remove ignored index (special tokens)\n",
                "    true_predictions = [\n",
                "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
                "        for prediction, label in zip(predictions, labels)\n",
                "    ]\n",
                "    true_labels = [\n",
                "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
                "        for prediction, label in zip(predictions, labels)\n",
                "    ]\n",
                "\n",
                "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
                "    return {\n",
                "        \"precision\": results[\"overall_precision\"],\n",
                "        \"recall\": results[\"overall_recall\"],\n",
                "        \"f1\": results[\"overall_f1\"],\n",
                "        \"accuracy\": results[\"overall_accuracy\"],\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "id": "b9c38f45",
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_accuracy(model, dataset, batch_size=16):\n",
                "    print(\"Evaluating model accuracy...\")\n",
                "    \n",
                "    # Use a separate Trainer for evaluation to leverage the DataCollator and efficient batching\n",
                "    eval_trainer = Trainer(\n",
                "        model=model,\n",
                "        args=TrainingArguments(output_dir=\"/tmp/eval\", per_device_eval_batch_size=batch_size, report_to=\"none\"),\n",
                "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
                "        eval_dataset=dataset,\n",
                "        compute_metrics=compute_metrics\n",
                "    )\n",
                "    \n",
                "    metrics = eval_trainer.evaluate()\n",
                "    print(\"Evaluation Results:\", metrics)\n",
                "    return metrics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "9065a308",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Baseline Evaluation\n",
                "# print(\"Running baseline evaluation on Test set...\")\n",
                "# baseline_metrics = evaluate_accuracy(model, tokenized_datasets[\"test\"])\n",
                "# print(f\"Baseline F1: {baseline_metrics['eval_f1']:.4f}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "id": "50d84a9e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing class weights...\n",
                        "Label counts: tensor([2.5629e+06, 8.4220e+03, 3.4713e+04, 7.3680e+03, 4.1920e+03, 5.0000e+00,\n",
                        "        1.5000e+01, 7.6260e+03, 1.3317e+04, 5.8470e+03, 2.9440e+03, 6.7600e+03,\n",
                        "        3.0992e+04, 8.6380e+03, 6.5280e+04, 9.7310e+03, 8.4153e+04, 6.9700e+02,\n",
                        "        7.1920e+03, 7.8590e+03, 1.2035e+04, 2.0920e+03, 3.2460e+03, 9.5460e+03,\n",
                        "        5.1252e+04, 8.0970e+03, 1.3896e+05, 9.0170e+03, 1.7882e+04, 2.2550e+03,\n",
                        "        4.5170e+03, 7.2600e+02, 1.5370e+03, 5.5670e+03, 3.4585e+04, 8.9760e+03,\n",
                        "        3.9170e+04, 7.4300e+03, 2.0002e+04, 3.1090e+03, 5.4890e+03, 7.4680e+03,\n",
                        "        2.2150e+03, 9.5650e+03, 5.7162e+04, 7.5610e+03, 1.7520e+03, 7.3270e+03,\n",
                        "        1.7878e+04, 7.2990e+03, 5.4982e+04, 1.4625e+04, 3.1667e+04, 7.5240e+03,\n",
                        "        2.7760e+03, 1.1084e+04, 5.4005e+04])\n",
                        "Class weights calculated. tensor([ 1.0000,  0.2348,  0.1000,  0.2679,  0.4662, 10.0000, 10.0000,  0.2590,\n",
                        "         0.1491,  0.3364,  0.6573,  0.2917,  0.1000,  0.2290,  0.1000,  0.2035,\n",
                        "         0.1000,  2.5104,  0.2744,  0.2514,  0.1649,  0.9128,  0.5980,  0.2074,\n",
                        "         0.1000,  0.2441,  0.1000,  0.2195,  0.1113,  0.8496,  0.4333,  2.4222,\n",
                        "         1.2222,  0.3531,  0.1000,  0.2204,  0.1000,  0.2657,  0.1000,  0.6235,\n",
                        "         0.3580,  0.2644,  0.8643,  0.2070,  0.1000,  0.2612,  1.0803,  0.2694,\n",
                        "         0.1113,  0.2704,  0.1000,  0.1359,  0.1000,  0.2624,  0.6957,  0.1789,\n",
                        "         0.1000])\n"
                    ]
                }
            ],
            "source": [
                "# 6. Class Imbalance Handling\n",
                "from torch.nn import CrossEntropyLoss\n",
                "\n",
                "def compute_class_weights(dataset, label2id):\n",
                "    print(\"Computing class weights...\")\n",
                "    label_counts = torch.zeros(len(label2id))\n",
                "    \n",
                "    # Iterate over train set to count labels\n",
                "    # Note: This might be slow if we iterate python-side.\n",
                "    # A faster way is roughly estimating or using the already known statistics if available.\n",
                "    # But let's do a quick pass or use a subset if needed. Here we do full pass.\n",
                "    for i, example in enumerate(dataset):\n",
                "        labels = example['labels']\n",
                "        for label in labels:\n",
                "            if label != -100:\n",
                "                label_counts[label] += 1\n",
                "    \n",
                "    print(f\"Label counts: {label_counts}\")\n",
                "    \n",
                "    # Inverse frequency with smoothing\n",
                "    weights = 1.0 / (label_counts + 100)\n",
                "    weights = weights / weights.sum() * len(label2id) # Normalize\n",
                "    \n",
                "    # Clamp to avoid extreme weights (e.g. for O vs rare B-tags)\n",
                "    weights = torch.clamp(weights, min=0.1, max=10.0)\n",
                "    weights[label2id[\"O\"]] = 1.0\n",
                "    \n",
                "    return weights\n",
                "\n",
                "# Compute weights using the training set\n",
                "class_weights = compute_class_weights(tokenized_datasets['train'], label2id)\n",
                "print(\"Class weights calculated.\", class_weights)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "id": "e09aeb6b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Custom Weighted Trainer\n",
                "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
                "class WeightedTrainer(Trainer):\n",
                "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
                "        labels = inputs.pop(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.logits\n",
                "        \n",
                "        # Use class weights injected into the loss function\n",
                "        loss_fct = CrossEntropyLoss(weight=class_weights.to(model.device), ignore_index=-100)\n",
                "        loss = loss_fct(logits.view(-1, len(label2id)), labels.view(-1))\n",
                "        \n",
                "        return (loss, outputs) if return_outputs else loss\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "399ea9c0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸš€ Training on GPU: NVIDIA GeForce RTX 2050\n",
                        "Starting Training...\n"
                    ]
                }
            ],
            "source": [
                "# 8. Training Configuration & Execution\n",
                "from transformers import EarlyStoppingCallback\n",
                "import torch\n",
                "import shutil\n",
                "\n",
                "# Verify GPU\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"ðŸš€ Training on GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"âš ï¸  WARNING: Training on CPU. This will be slow!\")\n",
                "\n",
                "args = TrainingArguments(\n",
                "    OUTPUT_DIR,\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=500,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=500,\n",
                "    save_total_limit=3,\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1\",\n",
                "    greater_is_better=True,\n",
                "    \n",
                "    learning_rate=2e-5, # Lower LR for fine-tuning\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    warmup_ratio=0.1,\n",
                "    num_train_epochs=4,\n",
                "    \n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=32,\n",
                "    gradient_accumulation_steps=2, # Effective batch size 32\n",
                "    \n",
                "    weight_decay=0.01,\n",
                "    fp16=torch.cuda.is_available(), # Use mixed precision if GPU available\n",
                "    dataloader_num_workers=4,\n",
                "    logging_steps=100,\n",
                "    report_to=\"none\", # Change to wandb/tensorboard if needed\n",
                "    seed=42\n",
                ")\n",
                "\n",
                "trainer = WeightedTrainer(\n",
                "    model=model,\n",
                "    args=args,\n",
                "    train_dataset=tokenized_datasets[\"train\"],\n",
                "    eval_dataset=tokenized_datasets[\"validation\"],\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "try:\n",
                "    trainer.train()\n",
                "    print(\"âœ… Training completed successfully.\")\n",
                "    trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\nðŸ›‘ Training interrupted by user. Saving checkpoint...\")\n",
                "    trainer.save_model(os.path.join(OUTPUT_DIR, \"interrupted_checkpoint\"))\n",
                "    print(\"Checkpoint saved.\")\n",
                "except Exception as e:\n",
                "    print(f\"\\nâŒ info: Training failed with error: {e}\")\n",
                "    # Attempt to save despite error\n",
                "    try:\n",
                "        trainer.save_model(os.path.join(OUTPUT_DIR, \"failed_checkpoint\"))\n",
                "        print(\"Crash checkpoint saved.\")\n",
                "    except:\n",
                "        print(\"Could not save crash checkpoint.\")\n",
                "    raise e\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f993f95d",
            "metadata": {},
            "source": [
                "## 6. Pruning & Quantization Logic\n",
                "Placeholders for subsequent steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0357b290",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pruning Placeholder\n",
                "# ...\n",
                "\n",
                "# Quantization Placeholder\n",
                "# ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f28b8952",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Verification Step\n",
                "# print(\"Verifying data format...\")\n",
                "# if len(tokenized_datasets['train']) > 0:\n",
                "#     sample = tokenized_datasets['train'][0]\n",
                "#     print(\"Keys:\", sample.keys())\n",
                "#     print(\"Input IDs length:\", len(sample['input_ids']))\n",
                "#     print(\"Labels length:\", len(sample['labels']))\n",
                "    \n",
                "#     # Check for -100\n",
                "#     print(\"Labels sample (first 20):\", sample['labels'][:20])\n",
                "    \n",
                "#     # Check for valid IDs\n",
                "#     import numpy as np\n",
                "#     labels = np.array(sample['labels'])\n",
                "#     print(\"Unique labels in sample:\", np.unique(labels))\n",
                "    \n",
                "#     # Check for any label ID that is not in label2id and not -100\n",
                "#     valid_ids = set(label2id.values())\n",
                "#     valid_ids.add(-100)\n",
                "#     invalid = [l for l in labels if l not in valid_ids]\n",
                "#     if invalid:\n",
                "#         print(f\"WARNING: Found invalid label IDs: {invalid}\")\n",
                "#     else:\n",
                "#         print(\"All label IDs are valid.\")\n",
                "# else:\n",
                "#     print(\"Training set is empty!\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
