{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213160f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate seqeval\n",
    "!pip install -q onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d570b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.nn.utils import prune\n",
    "import torch\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import copy\n",
    "import gc\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "from IPython.display import FileLink\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/hf\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/kaggle/working/hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94738e28-398e-4b5d-89fa-a62953af1488",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    \n",
    "    print(f\"‚úÖ GPU Found: {device_name}\")\n",
    "    print(f\"Memory: {total_mem / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå GPU NOT found. Check your drivers or Secure Boot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edcba9",
   "metadata": {},
   "source": [
    "# PII Masking Model Pipeline (Browser Optimized)\n",
    "\n",
    "This notebook implements a pipeline to create a small, efficient PII masking model using the Lottery Ticket Hypothesis (LTH) and Quantization.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Load Data**: `ai4privacy/pii-masking-300k` (Filter for **English** only).\n",
    "2. **Preprocessing**: Robust tokenization using character offsets to handle dataset quirks.\n",
    "3. **Save Initial Weights**: Critical for LTH \"rewinding\".\n",
    "4. **Fine-tune** -> **Prune** -> **Reset** -> **Retrain** -> **Quantize**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd54dd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
    "DATASET_NAME = \"ai4privacy/pii-masking-300k\"\n",
    "OUTPUT_DIR = \"/kaggle/working/pii_model_output\"\n",
    "INITIAL_WEIGHTS_PATH = os.path.join(OUTPUT_DIR, \"initial_weights.pt\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6e6cc",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Splits\n",
    "We load the dataset and filter for **English** (`language == 'English'`).\n",
    "Since the dataset only provides `train` and `validation` splits, we split `validation` into disjoint `validation` and `test` sets (50/50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49415fab",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(DATASET_NAME, trust_remote_code=True)\n",
    "dataset = dataset.filter(lambda x: x[\"language\"] == \"English\")\n",
    "\n",
    "if \"test\" not in dataset:\n",
    "    print(\"Creating Test split from Validation...\")\n",
    "    val_test_split = dataset[\"validation\"].train_test_split(test_size=0.5, seed=42)\n",
    "    dataset[\"validation\"] = val_test_split[\"train\"]\n",
    "    dataset[\"test\"] = val_test_split[\"test\"]\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345014d",
   "metadata": {},
   "source": [
    "## 2. Parsing & Label Extraction\n",
    "The dataset stores complex fields (`privacy_mask`) as JSON strings (in some versions) or list objects. We ensure they are parsed and extract unique labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea781e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_dataset_row(example):\n",
    "    if isinstance(example['privacy_mask'], str):\n",
    "        example['privacy_mask'] = json.loads(example['privacy_mask'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(parse_dataset_row)\n",
    "\n",
    "print(\"Extracting unique labels from Train split...\")\n",
    "unique_labels = set()\n",
    "for privacy_mask in dataset['train']['privacy_mask']:\n",
    "    for entity in privacy_mask:\n",
    "        unique_labels.add(entity['label'])\n",
    "\n",
    "label_list = [\"O\"]\n",
    "for label in sorted(list(unique_labels)):\n",
    "    label_list.append(f\"B-{label}\")\n",
    "    label_list.append(f\"I-{label}\")\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f\"Unique labels: {len(label_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d5a15",
   "metadata": {},
   "source": [
    "## 3. Robust Tokenization & Alignment\n",
    "We use `return_offsets_mapping=True` to map directly from character spans in `privacy_mask` to tokenizer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd60c4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"source_text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        stride=64,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False \n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = sample_mapping[i]\n",
    "        mask_list = examples[\"privacy_mask\"][sample_idx]\n",
    "        \n",
    "        chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][i])\n",
    "        \n",
    "        for entity in mask_list:\n",
    "            label_type = entity[\"label\"]\n",
    "            start_char = entity[\"start\"]\n",
    "            end_char = entity[\"end\"]\n",
    "            \n",
    "            b_id = label2id.get(f\"B-{label_type}\")\n",
    "            i_id = label2id.get(f\"I-{label_type}\")\n",
    "            if b_id is None: continue\n",
    "            \n",
    "            overlapping_indices = []\n",
    "            for idx, (t_start, t_end) in enumerate(offsets):\n",
    "                if t_start == 0 and t_end == 0: continue\n",
    "                if not (t_end <= start_char or t_start >= end_char):\n",
    "                    overlapping_indices.append(idx)\n",
    "            \n",
    "            for k, idx in enumerate(overlapping_indices):\n",
    "                current_label = chunk_labels[idx]\n",
    "                if current_label != label2id[\"O\"] and current_label != -100:\n",
    "                     continue\n",
    "                \n",
    "                if k == 0:\n",
    "                    t_start = offsets[idx][0]\n",
    "                    if start_char >= t_start:\n",
    "                        chunk_labels[idx] = b_id\n",
    "                    else:\n",
    "                        chunk_labels[idx] = i_id\n",
    "                else:\n",
    "                    chunk_labels[idx] = i_id\n",
    "        \n",
    "        for idx, (t_start, t_end) in enumerate(offsets):\n",
    "            if t_start == 0 and t_end == 0:\n",
    "                chunk_labels[idx] = -100\n",
    "        \n",
    "        labels.append(chunk_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c333fd",
   "metadata": {},
   "source": [
    "## 4. Save Initial Weights\n",
    "Save the untrained (or pre-trained base) weights to support the Lottery Ticket Hypothesis rewinding step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437618ba",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), INITIAL_WEIGHTS_PATH)\n",
    "print(f\"Initial weights saved to {INITIAL_WEIGHTS_PATH} with {len(label_list)} labels and sliding window configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323ee12",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c38f45",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataset, batch_size=16):\n",
    "    print(\"Evaluating model accuracy...\")\n",
    "    \n",
    "    eval_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=\"/tmp/eval\", per_device_eval_batch_size=batch_size, report_to=\"none\"),\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        eval_dataset=dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    metrics = eval_trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", metrics)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d84a9e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Class Imbalance Handling\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def compute_class_weights(dataset, label2id):\n",
    "    print(\"Computing class weights...\")\n",
    "    label_counts = torch.zeros(len(label2id))\n",
    "    \n",
    "   \n",
    "    for i, example in enumerate(dataset):\n",
    "        labels = example['labels']\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                label_counts[label] += 1\n",
    "    \n",
    "    print(f\"Label counts: {label_counts}\")\n",
    "    \n",
    "    weights = 1.0 / (label_counts + 100)\n",
    "    weights = weights / weights.sum() * len(label2id)\n",
    "\n",
    "    weights = torch.clamp(weights, min=0.1, max=10.0)\n",
    "    weights[label2id[\"O\"]] = 1.0\n",
    "    \n",
    "    return weights\n",
    "\n",
    "class_weights = compute_class_weights(tokenized_datasets['train'], label2id)\n",
    "print(\"Class weights calculated.\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09aeb6b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom Weighted Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(model.device), ignore_index=-100)\n",
    "        loss = loss_fct(logits.view(-1, len(label2id)), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ea9c0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration & Execution\n",
    "args = TrainingArguments(\n",
    "    OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    learning_rate=2e-5, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=4,\n",
    "    \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4, \n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    dataloader_num_workers=4,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\", \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully.\")\n",
    "    trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Training interrupted by user. Saving checkpoint...\")\n",
    "    trainer.save_model(os.path.join(OUTPUT_DIR, \"interrupted_checkpoint\"))\n",
    "    print(\"Checkpoint saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå info: Training failed with error: {e}\")\n",
    "    # Attempt to save despite error\n",
    "    try:\n",
    "        trainer.save_model(os.path.join(OUTPUT_DIR, \"failed_checkpoint\"))\n",
    "        print(\"Crash checkpoint saved.\")\n",
    "    except:\n",
    "        print(\"Could not save crash checkpoint.\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f993f95d",
   "metadata": {},
   "source": [
    "## 6. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357b290",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASELINE_MODEL_PATH = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "\n",
    "PRUNING_TARGETS = [0.25, 0.50, 0.75, 0.85] \n",
    "\n",
    "def get_sparsity(model):\n",
    "    \"\"\"Calculates the global sparsity of the model.\"\"\"\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if hasattr(module, \"weight_mask\"):\n",
    "                zeros = torch.sum(module.weight_mask == 0).item()\n",
    "            elif hasattr(module, \"weight\"):\n",
    "                zeros = torch.sum(module.weight == 0).item()\n",
    "            total_zeros += zeros\n",
    "            total_params += module.weight.nelement()\n",
    "    return total_zeros / total_params\n",
    "\n",
    "def strict_imp_loop():\n",
    "    print(f\"üöÄ Starting Strict IMP (Lottery Ticket Hypothesis)...\")\n",
    "    print(f\"üéØ Targets: {PRUNING_TARGETS}\")\n",
    "    \n",
    "    if not os.path.exists(INITIAL_WEIGHTS_PATH):\n",
    "        raise FileNotFoundError(f\"‚ùå Initial weights not found at {INITIAL_WEIGHTS_PATH}!\")\n",
    "    \n",
    "    w0_state_dict = torch.load(INITIAL_WEIGHTS_PATH, map_location=\"cpu\")\n",
    "    print(f\"‚úÖ Loaded Initial Weights (W0).\")\n",
    "\n",
    "    current_model_path = BASELINE_MODEL_PATH\n",
    "    current_sparsity = 0.0\n",
    "\n",
    "    for round_idx, target_sparsity in enumerate(PRUNING_TARGETS):\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"   üé´ ROUND {round_idx + 1}: Target {target_sparsity*100}% Sparsity\")\n",
    "        print(f\"=\"*50)\n",
    "\n",
    "        print(f\"üìÇ Loading model from: {current_model_path}\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(current_model_path)\n",
    "        model.to(\"cuda\")\n",
    "\n",
    "        if current_sparsity >= target_sparsity:\n",
    "            print(f\"‚ö†Ô∏è Already at {current_sparsity:.1%}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        amount_to_prune = (target_sparsity - current_sparsity) / (1.0 - current_sparsity)\n",
    "        print(f\"‚úÇÔ∏è  Pruning {amount_to_prune:.1%} of remaining parameters...\")\n",
    "\n",
    "        parameters_to_prune = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=amount_to_prune,\n",
    "        )\n",
    "\n",
    "        global_masks = {}\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                global_masks[name] = module.weight_mask.detach().cpu()\n",
    "\n",
    "                \n",
    "        print(\"‚ú® Rewinding to W0 (Initial Weights) and applying masks...\")\n",
    "        \n",
    "        model.load_state_dict(w0_state_dict, strict=False)\n",
    "        model.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, module in model.named_modules():\n",
    "                if name in global_masks:\n",
    "                    mask = global_masks[name].to(model.device)\n",
    "                    \n",
    "                    module.weight.mul_(mask)\n",
    "                    \n",
    "                    prune.custom_from_mask(module, name='weight', mask=mask)\n",
    "\n",
    "        current_sparsity = get_sparsity(model)\n",
    "        print(f\"‚úÖ Verified Sparsity: {current_sparsity:.2%}\")\n",
    "\n",
    "        run_dir = os.path.join(OUTPUT_DIR, f\"lth_sparsity_{int(target_sparsity*100)}\")\n",
    "        print(f\"üèãÔ∏è‚Äç‚ôÄÔ∏è Retraining Ticket... Output: {run_dir}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        round_args = copy.deepcopy(args)\n",
    "        round_args.output_dir = run_dir\n",
    "        round_args.learning_rate = 2e-5  \n",
    "        round_args.num_train_epochs = 4 \n",
    "        \n",
    "        trainer = WeightedTrainer(\n",
    "            model=model,\n",
    "            args=round_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        \n",
    "        print(\"üíæ Saving model for next round...\")\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "        trainer.save_model(run_dir)\n",
    "        tokenizer.save_pretrained(run_dir)\n",
    "        \n",
    "        current_model_path = run_dir\n",
    "        \n",
    "        del model, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nüéâ IMP Loop Completed Successfully!\")\n",
    "\n",
    "strict_imp_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31bc8e-78bb-4e8c-9e47-0420b3ee8162",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "BEST_MODEL_DIR = os.path.join(OUTPUT_DIR, \"lth_sparsity_85\") \n",
    "FINAL_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"pii_model_pruned_final\")\n",
    "\n",
    "print(f\"üìÇ Loading best winning ticket from: {BEST_MODEL_DIR}\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(BEST_MODEL_DIR)\n",
    "\n",
    "print(\"üî® Making pruning permanent (baking masks into weights)...\")\n",
    "\n",
    "layers_processed = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        if prune.is_pruned(module):\n",
    "            prune.remove(module, 'weight')\n",
    "            layers_processed += 1\n",
    "\n",
    "print(f\"‚úÖ Processed {layers_processed} layers. Masks are now removed.\")\n",
    "\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        if hasattr(module, 'weight_mask'):\n",
    "            print(f\"‚ö†Ô∏è WARNING: Layer {name} still has a mask!\")\n",
    "        \n",
    "\n",
    "print(f\"üíæ Saving final clean model to {FINAL_OUTPUT_DIR}...\")\n",
    "model.save_pretrained(FINAL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_OUTPUT_DIR)\n",
    "\n",
    "print(\"üéâ Success! The model is now standard architecture with zeroed weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb542ce2-b7e0-40f8-ba1d-9872179b616c",
   "metadata": {},
   "source": [
    "# 7. Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809d158-a350-4e02-856e-06e90bc5a680",
   "metadata": {},
   "source": [
    "## Export FP32 Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967787a4-de02-46c2-8286-4ccb79168f3e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PRUNED_MODEL_PATH = \"/kaggle/working/pii_model_output/pii_model_pruned_final\"  # From previous step\n",
    "ONNX_MODEL_PATH = \"/kaggle/working/pii_model.onnx\"\n",
    "QUANTIZED_ONNX_PATH = \"/kaggle/working/pii_model_quantized.onnx\"\n",
    "\n",
    "\n",
    "print(f\"üìÇ Loading model from {PRUNED_MODEL_PATH}...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(PRUNED_MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_PATH)\n",
    "model.eval() \n",
    "\n",
    "dummy_input_text = \"My name is John Doe.\"\n",
    "inputs = tokenizer(dummy_input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Export to ONNX\n",
    "print(f\"üîÑ Exporting to ONNX (FP32)...\")\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (inputs['input_ids'], inputs['attention_mask']), \n",
    "    ONNX_MODEL_PATH,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "    },\n",
    "    opset_version=14, \n",
    "    do_constant_folding=True\n",
    ")\n",
    "print(f\"‚úÖ Exported to {ONNX_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9f4a4-d2db-4cbc-9b2e-f29a0909d79d",
   "metadata": {},
   "source": [
    "## Quantize the ONNX Model (INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568564f-1a2f-4b29-b6f2-14fd9bb3b1d4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"üìâ Quantizing ONNX model to INT8...\")\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=ONNX_MODEL_PATH,\n",
    "    model_output=QUANTIZED_ONNX_PATH,\n",
    "    weight_type=QuantType.QUInt8 \n",
    ")\n",
    "\n",
    "size_fp32 = os.path.getsize(ONNX_MODEL_PATH) / (1024 * 1024)\n",
    "size_int8 = os.path.getsize(QUANTIZED_ONNX_PATH) / (1024 * 1024)\n",
    "\n",
    "print(f\"üéâ Done!\")\n",
    "print(f\"Original ONNX (FP32):   {size_fp32:.2f} MB\")\n",
    "print(f\"Quantized ONNX (INT8):  {size_int8:.2f} MB\")\n",
    "print(f\"üîª Reduction:            {100 - (size_int8 / size_fp32 * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44913f14-73a1-45a1-85c6-158ad4ec0128",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def evaluate_onnx(model_path, dataset, label_list):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÄÔ∏è Evaluating ONNX model: {model_path}\")\n",
    "    \n",
    "    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    input_name = session.get_inputs()[0].name\n",
    "    label_map = {i: label for i, label in enumerate(label_list)}\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(\"Running inference...\")\n",
    "    for batch in tqdm(dataset):\n",
    "        inputs = {\n",
    "            \"input_ids\": np.array([batch[\"input_ids\"]], dtype=np.int64),\n",
    "            \"attention_mask\": np.array([batch[\"attention_mask\"]], dtype=np.int64)\n",
    "        }\n",
    "        \n",
    "        outputs = session.run(None, inputs)[0] \n",
    "        preds = np.argmax(outputs, axis=2)[0] \n",
    "        \n",
    "       \n",
    "        true_labels = [label_map[l] for l in batch[\"labels\"] if l != -100]\n",
    "        true_preds = [label_map[p] for (p, l) in zip(preds, batch[\"labels\"]) if l != -100]\n",
    "        \n",
    "        predictions.append(true_preds)\n",
    "        references.append(true_labels)\n",
    "        \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1712710-0205-4b3b-ac78-12523be9dfb1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "SOURCE_MODEL_DIR = \"/kaggle/working/pii_model_output/pii_model_pruned_final\" \n",
    "QUANTIZED_MODEL_PATH = \"/kaggle/working/pii_model_quantized.onnx\"         \n",
    "EXPORT_DIR = \"/kaggle/working/browser_ready_pack\"\n",
    "\n",
    "if os.path.exists(EXPORT_DIR):\n",
    "    shutil.rmtree(EXPORT_DIR)\n",
    "os.makedirs(EXPORT_DIR)\n",
    "\n",
    "print(f\"üöÄ Preparing browser artifacts in: {EXPORT_DIR}\")\n",
    "\n",
    "dst_model_path = os.path.join(EXPORT_DIR, \"model.onnx\")\n",
    "shutil.copy(QUANTIZED_MODEL_PATH, dst_model_path)\n",
    "print(f\"‚úÖ Copied Model: {dst_model_path}\")\n",
    "\n",
    "tokenizer_files = [\n",
    "    \"tokenizer.json\", \n",
    "    \"tokenizer_config.json\", \n",
    "    \"special_tokens_map.json\", \n",
    "    \"vocab.txt\"\n",
    "]\n",
    "\n",
    "for filename in tokenizer_files:\n",
    "    src = os.path.join(SOURCE_MODEL_DIR, filename)\n",
    "    dst = os.path.join(EXPORT_DIR, filename)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"‚úÖ Copied Tokenizer: {filename}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: {filename} not found (some tokenizers don't use all of them).\")\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(SOURCE_MODEL_DIR, \"config.json\"), \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    if \"id2label\" in config_data:\n",
    "        labels_path = os.path.join(EXPORT_DIR, \"labels.json\")\n",
    "        with open(labels_path, \"w\") as f:\n",
    "            json.dump(config_data[\"id2label\"], f, indent=2)\n",
    "        print(f\"‚úÖ Extracted Label Map: labels.json\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'id2label' not found in config.json!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error extracting labels: {e}\")\n",
    "\n",
    "shutil.make_archive(\"/kaggle/working/pii_browser_pack\", 'zip', EXPORT_DIR)\n",
    "print(\"\\nüéâ Done! Download 'pii_browser_pack.zip' from the Output tab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb901d9-8c50-4104-bddf-6fcca6aee8e0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FileLink(r'pii_browser_pack.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abebb09-dcbb-46dd-b2bd-35b4129ab8cc",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Zipping model for download...\")\n",
    "shutil.make_archive(\"/kaggle/working/pii_model\", 'zip', os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "print(\"Done! You can now download pii_model.zip from the Output tab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf9d51-926f-4247-b77e-a0813ef467ed",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FileLink(r'pii_model.zip')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
