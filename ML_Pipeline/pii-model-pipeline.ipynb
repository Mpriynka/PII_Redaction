{"cells": [{"cell_type": "code", "execution_count": null, "id": "d213160f", "metadata": {"trusted": true}, "outputs": [], "source": ["!pip install -q evaluate seqeval\n", "!pip install -q optimum[onnxruntime] onnx onnxruntime"]}, {"cell_type": "code", "execution_count": null, "id": "f99d570b", "metadata": {"trusted": true}, "outputs": [], "source": ["import torch\n", "import os\n", "import json\n", "import numpy as np\n", "import evaluate\n", "from datasets import load_dataset\n", "from transformers import (\n", "    AutoTokenizer,\n", "    AutoModelForTokenClassification,\n", "    TrainingArguments,\n", "    Trainer,\n", "    DataCollatorForTokenClassification, \n", "    EarlyStoppingCallback\n", ")\n", "from optimum.onnxruntime import ORTModelForTokenClassification\n", "import torch\n", "import shutil\n", "from collections import Counter\n", "import copy\n", "import gc\n", "import onnxruntime as ort\n", "from tqdm import tqdm\n", "from IPython.display import FileLink\n", "\n", "os.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"\n", "os.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/hf\"\n", "os.environ[\"HF_DATASETS_CACHE\"] = \"/kaggle/working/hf\""]}, {"cell_type": "code", "execution_count": null, "id": "94738e28-398e-4b5d-89fa-a62953af1488", "metadata": {"trusted": true}, "outputs": [], "source": ["if torch.cuda.is_available():\n", "    device_name = torch.cuda.get_device_name(0)\n", "    total_mem = torch.cuda.get_device_properties(0).total_memory\n", "    \n", "    print(f\"\u2705 GPU Found: {device_name}\")\n", "    print(f\"Memory: {total_mem / 1024**3:.2f} GB\")\n", "else:\n", "    print(\"\u274c GPU NOT found. Check your drivers or Secure Boot.\")"]}, {"cell_type": "markdown", "id": "91edcba9", "metadata": {}, "source": ["# PII Masking Model Pipeline (Browser Optimized)\n", "\n", "This notebook implements a pipeline to create a small, efficient PII masking model using Quantization.\n", "\n", "**Key Steps:**\n", "1. **Load Data**: `ai4privacy/pii-masking-300k` (Filter for **English** only).\n", "2. **Preprocessing**: Robust tokenization using character offsets to handle dataset quirks.\n", "3. **Fine-tune** -> **Quantize**.\n"]}, {"cell_type": "code", "execution_count": null, "id": "1ddd54dd", "metadata": {"trusted": true}, "outputs": [], "source": ["# Configuration\n", "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n", "DATASET_NAME = \"ai4privacy/pii-masking-300k\"\n", "OUTPUT_DIR = \"/kaggle/working/pii_model_output\"\n", "\n", "os.makedirs(OUTPUT_DIR, exist_ok=True)"]}, {"cell_type": "markdown", "id": "52b6e6cc", "metadata": {}, "source": ["## 1. Data Loading & Splits\n", "We load the dataset and filter for **English** (`language == 'English'`).\n", "Since the dataset only provides `train` and `validation` splits, we split `validation` into disjoint `validation` and `test` sets (50/50)."]}, {"cell_type": "code", "execution_count": null, "id": "49415fab", "metadata": {"trusted": true}, "outputs": [], "source": ["# Load Dataset\n", "dataset = load_dataset(DATASET_NAME, trust_remote_code=True)\n", "dataset = dataset.filter(lambda x: x[\"language\"] == \"English\")\n", "\n", "if \"test\" not in dataset:\n", "    print(\"Creating Test split from Validation...\")\n", "    val_test_split = dataset[\"validation\"].train_test_split(test_size=0.5, seed=42)\n", "    dataset[\"validation\"] = val_test_split[\"train\"]\n", "    dataset[\"test\"] = val_test_split[\"test\"]\n", "\n", "print(dataset)\n"]}, {"cell_type": "markdown", "id": "a345014d", "metadata": {}, "source": ["## 2. Parsing & Label Extraction\n", "The dataset stores complex fields (`privacy_mask`) as JSON strings (in some versions) or list objects. We ensure they are parsed and extract unique labels."]}, {"cell_type": "code", "execution_count": null, "id": "76ea781e", "metadata": {"trusted": true}, "outputs": [], "source": ["def parse_dataset_row(example):\n", "    if isinstance(example['privacy_mask'], str):\n", "        example['privacy_mask'] = json.loads(example['privacy_mask'])\n", "    for entity in example['privacy_mask']:\n", "        entity['label'] = 'PII'\n", "    return example\n", "\n", "# We map once. This is cached by HF datasets so subsequent calls are fast.\n", "dataset = dataset.map(parse_dataset_row)\n", "\n", "print(\"Mapping labels to O, B-PII, I-PII...\")\n", "label_list = [\"O\", \"B-PII\", \"I-PII\"]\n", "\n", "label2id = {label: i for i, label in enumerate(label_list)}\n", "id2label = {i: label for label, i in label2id.items()}\n", "print(f\"Unique labels: {len(label_list)}\")\n"]}, {"cell_type": "markdown", "id": "597d5a15", "metadata": {}, "source": ["## 3. Robust Tokenization & Alignment\n", "We use `return_offsets_mapping=True` to map directly from character spans in `privacy_mask` to tokenizer tokens."]}, {"cell_type": "code", "execution_count": null, "id": "f9fd60c4", "metadata": {"trusted": true}, "outputs": [], "source": ["tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n", "\n", "def tokenize_and_align_labels(examples):\n", "    tokenized_inputs = tokenizer(\n", "        examples[\"source_text\"],\n", "        truncation=True,\n", "        max_length=512,\n", "        stride=64,\n", "        return_overflowing_tokens=True,\n", "        return_offsets_mapping=True,\n", "        padding=False # Dynamic padding in DataCollator\n", "    )\n", "\n", "    labels = []\n", "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n", "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n", "\n", "    for i, offsets in enumerate(offset_mapping):\n", "        sample_idx = sample_mapping[i]\n", "        mask_list = examples[\"privacy_mask\"][sample_idx]\n", "        \n", "        # Initialize with O\n", "        chunk_labels = [label2id[\"O\"]] * len(tokenized_inputs[\"input_ids\"][i])\n", "        \n", "        for entity in mask_list:\n", "            label_type = entity[\"label\"]\n", "            start_char = entity[\"start\"]\n", "            end_char = entity[\"end\"]\n", "            \n", "            b_id = label2id.get(f\"B-{label_type}\")\n", "            i_id = label2id.get(f\"I-{label_type}\")\n", "            if b_id is None: continue\n", "            \n", "            # Find all overlapping tokens for this entity in this chunk\n", "            # Overlap condition: not (token_end <= entity_start or token_start >= entity_end)\n", "            overlapping_indices = []\n", "            for idx, (t_start, t_end) in enumerate(offsets):\n", "                if t_start == 0 and t_end == 0: continue\n", "                if not (t_end <= start_char or t_start >= end_char):\n", "                    overlapping_indices.append(idx)\n", "            \n", "            # Assign Labels\n", "            for k, idx in enumerate(overlapping_indices):\n", "                current_label = chunk_labels[idx]\n", "                # Don't overwrite existing B/I labels unless they are O or -100\n", "                if current_label != label2id[\"O\"] and current_label != -100:\n", "                     continue\n", "                \n", "                # Logic: The FIRST overlapping token gets B, others I.\n", "                # UNLESS the entity actually started before this token (Sliding window split).\n", "                if k == 0:\n", "                    t_start = offsets[idx][0]\n", "                    # If entity start is >= token start, it means this token contains the start -> B\n", "                    if start_char >= t_start:\n", "                        chunk_labels[idx] = b_id\n", "                    else:\n", "                        # Entity started before this token -> I\n", "                        chunk_labels[idx] = i_id\n", "                else:\n", "                    chunk_labels[idx] = i_id\n", "        \n", "        # Mask special tokens\n", "        for idx, (t_start, t_end) in enumerate(offsets):\n", "            if t_start == 0 and t_end == 0:\n", "                chunk_labels[idx] = -100\n", "        \n", "        labels.append(chunk_labels)\n", "\n", "    tokenized_inputs[\"labels\"] = labels\n", "    return tokenized_inputs\n", "\n", "tokenized_datasets = dataset.map(\n", "    tokenize_and_align_labels, \n", "    batched=True, \n", "    remove_columns=dataset[\"train\"].column_names\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "init_model", "metadata": {"trusted": true}, "outputs": [], "source": ["model = AutoModelForTokenClassification.from_pretrained(\n", "    MODEL_CHECKPOINT, \n", "    num_labels=len(label_list), \n", "    id2label=id2label, \n", "    label2id=label2id\n", ")\n", "print(f\"Model initialized with {len(label_list)} labels and sliding window configuration.\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "8323ee12", "metadata": {"trusted": true}, "outputs": [], "source": ["# Evaluation Metrics\n", "metric = evaluate.load(\"seqeval\")\n", "\n", "def compute_metrics(p):\n", "    predictions, labels = p\n", "    predictions = np.argmax(predictions, axis=2)\n", "\n", "    # Remove ignored index (special tokens)\n", "    true_predictions = [\n", "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n", "        for prediction, label in zip(predictions, labels)\n", "    ]\n", "    true_labels = [\n", "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n", "        for prediction, label in zip(predictions, labels)\n", "    ]\n", "\n", "    results = metric.compute(predictions=true_predictions, references=true_labels)\n", "    return {\n", "        \"precision\": results[\"overall_precision\"],\n", "        \"recall\": results[\"overall_recall\"],\n", "        \"f1\": results[\"overall_f1\"],\n", "        \"accuracy\": results[\"overall_accuracy\"],\n", "    }\n"]}, {"cell_type": "code", "execution_count": null, "id": "b9c38f45", "metadata": {"trusted": true}, "outputs": [], "source": ["def evaluate_accuracy(model, dataset, batch_size=16):\n", "    print(\"Evaluating model accuracy...\")\n", "    \n", "    # Use a separate Trainer for evaluation to leverage the DataCollator and efficient batching\n", "    eval_trainer = Trainer(\n", "        model=model,\n", "        args=TrainingArguments(output_dir=\"/tmp/eval\", per_device_eval_batch_size=batch_size, report_to=\"none\"),\n", "        data_collator=DataCollatorForTokenClassification(tokenizer),\n", "        eval_dataset=dataset,\n", "        compute_metrics=compute_metrics\n", "    )\n", "    \n", "    metrics = eval_trainer.evaluate()\n", "    print(\"Evaluation Results:\", metrics)\n", "    return metrics\n"]}, {"cell_type": "code", "execution_count": null, "id": "50d84a9e", "metadata": {"trusted": true}, "outputs": [], "source": ["# 6. Class Imbalance Handling\n", "from torch.nn import CrossEntropyLoss\n", "\n", "def compute_class_weights(dataset, label2id):\n", "    print(\"Computing class weights...\")\n", "    label_counts = torch.zeros(len(label2id))\n", "    \n", "    # Iterate over train set to count labels\n", "    # Note: This might be slow if we iterate python-side.\n", "    # A faster way is roughly estimating or using the already known statistics if available.\n", "    # But let's do a quick pass or use a subset if needed. Here we do full pass.\n", "    for i, example in enumerate(dataset):\n", "        labels = example['labels']\n", "        for label in labels:\n", "            if label != -100:\n", "                label_counts[label] += 1\n", "    \n", "    print(f\"Label counts: {label_counts}\")\n", "    \n", "    # Inverse frequency with smoothing\n", "    weights = 1.0 / (label_counts + 100)\n", "    weights = weights / weights.sum() * len(label2id) # Normalize\n", "    \n", "    # Clamp to avoid extreme weights (e.g. for O vs rare B-tags)\n", "    weights = torch.clamp(weights, min=0.1, max=10.0)\n", "    weights[label2id[\"O\"]] = 1.0\n", "    \n", "    return weights\n", "\n", "# Compute weights using the training set\n", "class_weights = compute_class_weights(tokenized_datasets['train'], label2id)\n", "print(\"Class weights calculated.\", class_weights)\n"]}, {"cell_type": "code", "execution_count": null, "id": "e09aeb6b", "metadata": {"trusted": true}, "outputs": [], "source": ["# 7. Custom Weighted Trainer\n", "data_collator = DataCollatorForTokenClassification(tokenizer)\n", "class WeightedTrainer(Trainer):\n", "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n", "        labels = inputs.pop(\"labels\")\n", "        outputs = model(**inputs)\n", "        logits = outputs.logits\n", "        \n", "        # Use class weights injected into the loss function\n", "        loss_fct = CrossEntropyLoss(weight=class_weights.to(model.device), ignore_index=-100)\n", "        loss = loss_fct(logits.view(-1, len(label2id)), labels.view(-1))\n", "        \n", "        return (loss, outputs) if return_outputs else loss\n"]}, {"cell_type": "code", "execution_count": null, "id": "399ea9c0", "metadata": {"trusted": true}, "outputs": [], "source": ["# 8. Training Configuration & Execution\n", "args = TrainingArguments(\n", "    OUTPUT_DIR,\n", "    eval_strategy=\"steps\",\n", "    eval_steps=500,\n", "    save_strategy=\"steps\",\n", "    save_steps=500,\n", "    save_total_limit=3,\n", "    load_best_model_at_end=True,\n", "    metric_for_best_model=\"f1\",\n", "    greater_is_better=True,\n", "    \n", "    learning_rate=2e-5, \n", "    lr_scheduler_type=\"cosine\",\n", "    warmup_ratio=0.1,\n", "    num_train_epochs=4,\n", "    \n", "    per_device_train_batch_size=4,\n", "    per_device_eval_batch_size=16,\n", "    gradient_accumulation_steps=4, \n", "    \n", "    weight_decay=0.01,\n", "    fp16=torch.cuda.is_available(), \n", "    dataloader_num_workers=4,\n", "    logging_steps=100,\n", "    report_to=\"none\", \n", "    seed=42\n", ")\n", "\n", "trainer = WeightedTrainer(\n", "    model=model,\n", "    args=args,\n", "    train_dataset=tokenized_datasets[\"train\"],\n", "    eval_dataset=tokenized_datasets[\"validation\"],\n", "    data_collator=data_collator,\n", "    compute_metrics=compute_metrics,\n", "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n", ")\n", "\n", "print(\"Starting Training...\")\n", "try:\n", "    trainer.train()\n", "    print(\"\u2705 Training completed successfully.\")\n", "    trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n", "except KeyboardInterrupt:\n", "    print(\"\\n\ud83d\uded1 Training interrupted by user. Saving checkpoint...\")\n", "    trainer.save_model(os.path.join(OUTPUT_DIR, \"interrupted_checkpoint\"))\n", "    print(\"Checkpoint saved.\")\n", "except Exception as e:\n", "    print(f\"\\n\u274c info: Training failed with error: {e}\")\n", "    # Attempt to save despite error\n", "    try:\n", "        trainer.save_model(os.path.join(OUTPUT_DIR, \"failed_checkpoint\"))\n", "        print(\"Crash checkpoint saved.\")\n", "    except:\n", "        print(\"Could not save crash checkpoint.\")\n", "    raise e\n"]}, {"cell_type": "markdown", "id": "bb542ce2-b7e0-40f8-ba1d-9872179b616c", "metadata": {}, "source": ["# 7. Quantization"]}, {"cell_type": "markdown", "id": "3809d158-a350-4e02-856e-06e90bc5a680", "metadata": {}, "source": ["## Export Model and Metadata to ONNX"]}, {"cell_type": "code", "execution_count": null, "id": "967787a4-de02-46c2-8286-4ccb79168f3e", "metadata": {"trusted": true}, "outputs": [], "source": ["# Configuration\n", "FINAL_MODEL_PATH = \"/kaggle/working/pii_model_output/final_model\"\n", "EXPORT_DIR = \"/kaggle/working/browser_ready_pack\"\n", "\n", "import os\n", "from optimum.onnxruntime import ORTQuantizer\n", "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n", "os.makedirs(EXPORT_DIR, exist_ok=True)\n", "\n", "# 1. Export to ONNX using Optimum (This includes config.json and tokenizer files automatically)\n", "print(f\"\ud83d\udd04 Exporting model to ONNX via Optimum...\")\n", "ort_model = ORTModelForTokenClassification.from_pretrained(FINAL_MODEL_PATH, export=True)\n", "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_PATH)\n", "\n", "# Save the FP32 model, config.json, and tokenizer files to the EXPORT_DIR\n", "ort_model.save_pretrained(EXPORT_DIR)\n", "tokenizer.save_pretrained(EXPORT_DIR)\n", "print(f\"\u2705 Exported FP32 model and metadata to {EXPORT_DIR}\")\n"]}, {"cell_type": "markdown", "id": "a0e9f4a4-d2db-4cbc-9b2e-f29a0909d79d", "metadata": {}, "source": ["## Quantize the ONNX Model (INT8)"]}, {"cell_type": "code", "execution_count": null, "id": "5568564f-1a2f-4b29-b6f2-14fd9bb3b1d4", "metadata": {"trusted": true}, "outputs": [], "source": ["print(f\"\ud83d\udcc9 Quantizing ONNX model to INT8 via Optimum...\")\n", "\n", "EXPORT_DIR = \"/kaggle/working/browser_ready_pack\"\n", "\n", "# Use the ORTQuantizer from Optimum for proper graph optimization\n", "quantizer = ORTQuantizer.from_pretrained(EXPORT_DIR, file_name=\"model.onnx\")\n", "\n", "# Create a quantization configuration (Dynamic int8 is best for CPU/browser)\n", "dqconfig = AutoQuantizationConfig.avx2(is_static=False, per_channel=False)\n", "\n", "# This will optimize the graph, quantize it, and save it as `model_quantized.onnx`\n", "quantizer.quantize(save_dir=EXPORT_DIR, quantization_config=dqconfig)\n", "\n", "# We can safely delete the original FP32 model now to save space\n", "# The browser only needs the `model_quantized.onnx`\n", "original_model_path = os.path.join(EXPORT_DIR, \"model.onnx\")\n", "if os.path.exists(original_model_path):\n", "    os.remove(original_model_path)\n", "    print(\"\ud83d\uddd1\ufe0f Deleted original FP32 model.onnx to reduce bundle size.\")\n", "\n", "print(f\"\ud83c\udf89 Done! The optimized INT8 model is saved in {EXPORT_DIR}\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "44913f14-73a1-45a1-85c6-158ad4ec0128", "metadata": {"trusted": true}, "outputs": [], "source": ["# Load the metric\n", "metric = evaluate.load(\"seqeval\")\n", "\n", "def evaluate_onnx(model_path, dataset, label_list):\n", "    print(f\"\ud83d\udd75\ufe0f\u200d\u2640\ufe0f Evaluating ONNX model: {model_path}\")\n", "    \n", "    # Create ONNX Runtime session\n", "    # providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if you have GPU, else just CPU\n", "    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n", "    \n", "    input_name = session.get_inputs()[0].name\n", "    label_map = {i: label for i, label in enumerate(label_list)}\n", "    \n", "    predictions = []\n", "    references = []\n", "    \n", "    print(\"Running inference...\")\n", "    for batch in tqdm(dataset):\n", "        # Prepare inputs for ONNX (needs numpy arrays)\n", "        inputs = {\n", "            \"input_ids\": np.array([batch[\"input_ids\"]], dtype=np.int64),\n", "            \"attention_mask\": np.array([batch[\"attention_mask\"]], dtype=np.int64)\n", "        }\n", "        \n", "        # Run inference\n", "        outputs = session.run(None, inputs)[0] # [batch, seq_len, num_labels]\n", "        preds = np.argmax(outputs, axis=2)[0]  # [seq_len]\n", "        \n", "        # Align predictions with labels (filtering out -100)\n", "        true_labels = [label_map[l] for l in batch[\"labels\"] if l != -100]\n", "        true_preds = [label_map[p] for (p, l) in zip(preds, batch[\"labels\"]) if l != -100]\n", "        \n", "        predictions.append(true_preds)\n", "        references.append(true_labels)\n", "        \n", "    results = metric.compute(predictions=predictions, references=references)\n", "    return results\n", "\n", "print(\"Evaluating INT8 Model...\")\n", "label_list = [\"O\", \"B-PII\", \"I-PII\"]\n", "onnx_results = evaluate_onnx(QUANTIZED_ONNX_PATH, tokenized_datasets[\"test\"], label_list)\n", "print(\"ONNX Results:\", onnx_results)\n"]}, {"cell_type": "code", "execution_count": null, "id": "b1712710-0205-4b3b-ac78-12523be9dfb1", "metadata": {"trusted": true}, "outputs": [], "source": ["import shutil\n", "import os\n", "\n", "EXPORT_DIR = \"/kaggle/working/browser_ready_pack\"\n", "\n", "# The EXPORT_DIR already contains the FP32 ONNX model, config.json, tokenizer files, \n", "# and the new model_quantized.onnx. It is completely ready for Transformers.js!\n", "\n", "# 5. Zip it up!\n", "print(\"\\n\ud83d\udce6 Zipping the browser-ready pack...\")\n", "shutil.make_archive(\"/kaggle/working/pii_browser_pack\", 'zip', EXPORT_DIR)\n", "print(\"\ud83c\udf89 Done! Download 'pii_browser_pack.zip' from the Output tab.\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "9fb901d9-8c50-4104-bddf-6fcca6aee8e0", "metadata": {"trusted": true}, "outputs": [], "source": ["FileLink(r'pii_browser_pack.zip')"]}, {"cell_type": "code", "execution_count": null, "id": "8abebb09-dcbb-46dd-b2bd-35b4129ab8cc", "metadata": {"trusted": true}, "outputs": [], "source": ["print(\"Zipping model for download...\")\n", "shutil.make_archive(\"/kaggle/working/pii_model\", 'zip', os.path.join(OUTPUT_DIR, \"final_model\"))\n", "print(\"Done! You can now download pii_model.zip from the Output tab.\")"]}, {"cell_type": "code", "execution_count": null, "id": "62bf9d51-926f-4247-b77e-a0813ef467ed", "metadata": {"trusted": true}, "outputs": [], "source": ["FileLink(r'pii_model.zip')"]}], "metadata": {"kaggle": {"accelerator": "none", "dataSources": [], "dockerImageVersionId": 31260, "isGpuEnabled": false, "isInternetEnabled": true, "language": "python", "sourceType": "notebook"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.12"}}, "nbformat": 4, "nbformat_minor": 5}